# Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment

[**Paper**](https://openreview.net/pdf?id=jkcHYEfPv3) | [**Github**](https://github.com/declare-lab/red-instruct) | [**Dataset**](https://huggingface.co/datasets/declare-lab/HarmfulQA) | [**Model**](https://huggingface.co/declare-lab/starling-7B)

ðŸ“¢ Training and evaluation codes will be released soon. Stay tuned!

